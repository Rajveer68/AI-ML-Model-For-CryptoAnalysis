{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa193062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76e3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "df = pd.read_csv('../dataset/crypto_sentiment_prediction_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e492836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feature Engineering: Create Target Variable\n",
    "# Target = 1 if Price Change > 0 (Bullish), else 0 (Bearish)\n",
    "df['target'] = (df['price_change_24h_percent'] > 0).astype(int)\n",
    "\n",
    "# 2. Select Features and Drop Leakage/Redundant Columns\n",
    "X = df.drop(columns=['timestamp', 'price_change_24h_percent', \n",
    "                     'prediction_confidence', 'target'])\n",
    "y = df['target']\n",
    "\n",
    "# 3. Define Preprocessing Pipeline\n",
    "# Scale numerical features and One-Hot Encode categorical features\n",
    "categorical_features = ['cryptocurrency']\n",
    "numeric_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda9f395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation keys: ['price_change_24h_percent', 'target', 'market_cap_usd', 'current_price_usd', 'rsi_technical_indicator', 'volatility_index', 'news_sentiment_score', 'social_sentiment_score', 'prediction_confidence', 'trading_volume_24h', 'news_impact_score', 'fear_greed_index', 'social_mentions_count']\n",
      "social_sentiment_score Correlation: 0.0106\n",
      "fear_greed_index Correlation: -0.0152\n"
     ]
    }
   ],
   "source": [
    "# Safely compute Pearson correlations using numeric columns only\n",
    "num_df = df.select_dtypes(include=[np.number])\n",
    "correlations = num_df.corr()['price_change_24h_percent'].sort_values(ascending=False)\n",
    "\n",
    "# Debug: show available correlation keys\n",
    "print(\"Correlation keys:\", list(correlations.index))\n",
    "\n",
    "# Safely print specific sentiment correlations\n",
    "for key in ['social_sentiment_score', 'fear_greed_index']:\n",
    "    val = correlations.get(key)\n",
    "    if val is None or pd.isna(val):\n",
    "        print(f\"{key}: missing or non-numeric (NaN)\")\n",
    "    else:\n",
    "        # format float safely\n",
    "        print(f\"{key} Correlation: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2536a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train-Test Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84a8464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.4964\n",
      "Random Forest Accuracy: 0.4964\n",
      "XGBoost Accuracy: 0.4673\n",
      "SVM Accuracy: 0.4576\n"
     ]
    }
   ],
   "source": [
    "# Define the four baseline models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Training Loop using the Pipeline\n",
    "for name, model in models.items():\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('classifier', model)])\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e75795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Model Results ---\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.4964\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.4964\n",
      "\n",
      "Model: XGBoost\n",
      "Accuracy: 0.4673\n",
      "\n",
      "Model: SVM\n",
      "Accuracy: 0.4576\n"
     ]
    }
   ],
   "source": [
    "# 6. Training Loop & Initial Evaluation\n",
    "print(\"--- Baseline Model Results ---\")\n",
    "for name, model in models.items():\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('classifier', model)])\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29caa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comprehensive Hyperparameter Tuning ---\n",
      "\n",
      "Tuning Logistic Regression...\n",
      "Best CV Score: 0.5067\n",
      "Test Set Accuracy: 0.4939\n",
      "Best Params: {'classifier__C': 0.01, 'classifier__solver': 'liblinear'}\n",
      "\n",
      "Tuning Random Forest...\n",
      "Best CV Score: 0.5055\n",
      "Test Set Accuracy: 0.5400\n",
      "Best Params: {'classifier__max_depth': 20, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 50}\n",
      "\n",
      "Tuning XGBoost...\n",
      "Best CV Score: 0.5133\n",
      "Test Set Accuracy: 0.4673\n",
      "Best Params: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
      "\n",
      "Tuning SVM...\n",
      "Best CV Score: 0.5152\n",
      "Test Set Accuracy: 0.4625\n",
      "Best Params: {'classifier__C': 10, 'classifier__gamma': 'auto', 'classifier__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grids for all models\n",
    "# Note: 'classifier__' prefix is required to access model params inside the Pipeline\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__solver': ['liblinear']  # Good for small datasets\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__kernel': ['rbf', 'linear'],\n",
    "        'classifier__gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"--- Comprehensive Hyperparameter Tuning ---\")\n",
    "best_models = {}\n",
    "\n",
    "# Loop through each model and perform GridSearchCV\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model)])\n",
    "    \n",
    "    # Skip tuning if model not in our grid definition (safety check)\n",
    "    if name in param_grids:\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "        grid = GridSearchCV(pipeline, param_grids[name], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        best_score = grid.best_score_\n",
    "        best_params = grid.best_params_\n",
    "        test_acc = accuracy_score(y_test, grid.best_estimator_.predict(X_test))\n",
    "        \n",
    "        best_models[name] = {'test_accuracy': test_acc, 'best_params': best_params}\n",
    "        print(f\"Best CV Score: {best_score:.4f}\")\n",
    "        print(f\"Test Set Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Best Params: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
