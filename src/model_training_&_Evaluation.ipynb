{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d2ffbe",
   "metadata": {},
   "source": [
    "# CRYPTO SENTIMENT PRESICTION DATASET - MACHINE LEARNING MODEL TRAINING AND EVALUATION\n",
    "\n",
    "**NOTEBOOK IS WRITTEN BY :-**\n",
    "<br></br>\n",
    "**AMOGH DATH KALASAPURA ARUNKUMAR : amoghdath.kalasapuraarunkumar@mail.bcu.ac.uk**\n",
    "\n",
    "\n",
    "**GITHUB REPO LINK** :- [https://github.com/Amogh-007-Rin/AI-ML-Model-For-CryptoAnalysis]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa193062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfcc8e",
   "metadata": {},
   "source": [
    "**Code Explanation :-**\n",
    "\n",
    "This cell imports all necessary libraries for model building and evaluation:\n",
    "- **Data Manipulation**: `pandas` and `numpy` for data processing\n",
    "- **Visualization**: `matplotlib` and `seaborn` for plotting\n",
    "- **Model Selection & Preprocessing**: `train_test_split`, `GridSearchCV`, `StandardScaler`, `OneHotEncoder`, `ColumnTransformer`, and `Pipeline` from scikit-learn\n",
    "- **Classification Models**: `LogisticRegression`, `RandomForestClassifier`, `GradientBoostingClassifier`, and `SVM`\n",
    "- **Metrics**: `classification_report`, `accuracy_score`, and `confusion_matrix` for model evaluation\n",
    "\n",
    "**Result Discussion :-**\n",
    "\n",
    "No output is produced as these are just library imports. All required ML libraries are loaded into the Python environment for subsequent model development and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76e3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Dataset\n",
    "df = pd.read_csv('../dataset/pre-processed-crypto-dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688fd611",
   "metadata": {},
   "source": [
    "**Code Explanation :-**\n",
    "\n",
    "This cell loads the cryptocurrency sentiment prediction dataset from a CSV file located in the parent directory's `dataset` folder. The dataset is read into a pandas DataFrame named `df` for further exploration and processing.\n",
    "\n",
    "**Result Discussion :-**\n",
    "\n",
    "The dataset is successfully loaded into memory. The DataFrame `df` now contains all rows and columns from the CSV file, ready for feature engineering and preprocessing in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e492836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feature Engineering: Create Target Variable\n",
    "# Target = 1 if Price Change > 0 (Bullish), else 0 (Bearish)\n",
    "df['target'] = (df['price_change_24h_percent'] > 0).astype(int)\n",
    "\n",
    "# 2. Select Features and Drop Leakage/Redundant Columns\n",
    "X = df.drop(columns=['timestamp', 'price_change_24h_percent', \n",
    "                     'prediction_confidence', 'target'])\n",
    "y = df['target']\n",
    "\n",
    "# 3. Define Preprocessing Pipeline\n",
    "# Scale numerical features and One-Hot Encode categorical features\n",
    "categorical_features = ['cryptocurrency']\n",
    "numeric_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation :-**\n",
    "\n",
    "This cell performs three key tasks:\n",
    "1. **Target Variable Creation**: Creates a binary target variable where 1 = bullish (price change > 0%) and 0 = bearish (price change â‰¤ 0%)\n",
    "2. **Feature Selection**: Drops irrelevant columns (`timestamp`, `price_change_24h_percent`, `prediction_confidence`) and separates features (X) from the target (y)\n",
    "3. **Preprocessing Pipeline**: Creates a `ColumnTransformer` that:\n",
    "   - Scales numeric features using `StandardScaler` (standardization)\n",
    "   - One-Hot Encodes categorical features (`cryptocurrency`) using `OneHotEncoder`\n",
    "\n",
    "**Result Discussion :-**\n",
    "\n",
    "The feature engineering produces a clean dataset where:\n",
    "- Target variable is binary (0 or 1), suitable for classification\n",
    "- Data leakage is prevented by removing price-related columns\n",
    "- Features are separated into X and y for model training\n",
    "- The preprocessing pipeline is ready to be used in ML models to ensure consistent feature transformation across training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dda9f395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation keys: ['price_change_24h_percent', 'target', 'market_cap_usd', 'current_price_usd', 'rsi_technical_indicator', 'volatility_index', 'news_sentiment_score', 'social_sentiment_score', 'prediction_confidence', 'trading_volume_24h', 'news_impact_score', 'fear_greed_index', 'social_mentions_count']\n",
      "social_sentiment_score Correlation: 0.0106\n",
      "fear_greed_index Correlation: -0.0152\n"
     ]
    }
   ],
   "source": [
    "# Safely compute Pearson correlations using numeric columns only\n",
    "num_df = df.select_dtypes(include=[np.number])\n",
    "correlations = num_df.corr()['price_change_24h_percent'].sort_values(ascending=False)\n",
    "\n",
    "# Debug: show available correlation keys\n",
    "print(\"Correlation keys:\", list(correlations.index))\n",
    "\n",
    "# Safely print specific sentiment correlations\n",
    "for key in ['social_sentiment_score', 'fear_greed_index']:\n",
    "    val = correlations.get(key)\n",
    "    if val is None or pd.isna(val):\n",
    "        print(f\"{key}: missing or non-numeric (NaN)\")\n",
    "    else:\n",
    "        # format float safely\n",
    "        print(f\"{key} Correlation: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f244f",
   "metadata": {},
   "source": [
    "**Code Explanation :-**\n",
    "\n",
    "This cell analyzes feature correlations with the target variable (`price_change_24h_percent`):\n",
    "1. Selects only numeric columns from the dataset\n",
    "2. Computes Pearson correlation coefficients with `price_change_24h_percent`\n",
    "3. Sorts correlations in descending order\n",
    "4. Safely prints specific sentiment correlations (`social_sentiment_score`, `fear_greed_index`) while handling missing or non-numeric values\n",
    "\n",
    "**Result Discussion :-**\n",
    "\n",
    "The correlation analysis identifies which features have the strongest relationships with cryptocurrency price changes. The output shows:\n",
    "- All available numeric features and their correlation strengths\n",
    "- Specific sentiment indicators and their predictive power for price movements\n",
    "- Handles missing data gracefully by checking for NaN values before printing\n",
    "- Results inform feature importance and help validate the sentiment-price relationship hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2536a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train-Test Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de243977",
   "metadata": {},
   "source": [
    "**Code Explanation :-**\n",
    "\n",
    "This cell splits the dataset into training and testing subsets:\n",
    "- **80/20 Split**: 80% of data for training, 20% for testing\n",
    "- **Stratification**: Ensures balanced class distribution in both train and test sets (important for imbalanced datasets)\n",
    "- **Random State**: Fixed seed (42) ensures reproducibility across multiple runs\n",
    "- Produces four outputs: `X_train`, `X_test`, `y_train`, `y_test`\n",
    "\n",
    "**Result Discussion :-**\n",
    "\n",
    "The data is now partitioned such that:\n",
    "- Training set is used to fit model parameters\n",
    "- Test set is held out to evaluate model generalization performance\n",
    "- Stratification prevents skewed class distributions that could bias results\n",
    "- Reproducibility is ensured, allowing consistent experiments across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e84a8464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.4964\n",
      "Random Forest Accuracy: 0.4964\n",
      "XGBoost Accuracy: 0.4673\n",
      "SVM Accuracy: 0.4576\n"
     ]
    }
   ],
   "source": [
    "# Define the four baseline models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Training Loop using the Pipeline\n",
    "for name, model in models.items():\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('classifier', model)])\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"{name} Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ec173",
   "metadata": {},
   "source": [
    "**Code Explanation :-**\n",
    "\n",
    "This cell creates and trains four baseline classification models:\n",
    "1. **Logistic Regression**: Linear probabilistic classifier with max iterations set to 1000\n",
    "2. **Random Forest**: Ensemble of decision trees\n",
    "3. **XGBoost** (Gradient Boosting): Sequential tree-based ensemble\n",
    "4. **SVM**: Support Vector Machine classifier\n",
    "\n",
    "Each model is wrapped in a `Pipeline` that automatically applies the preprocessing transformations before classification. Models are trained on `X_train` and evaluated on `X_test`.\n",
    "\n",
    "**Result Discussion :-**\n",
    "\n",
    "The output shows accuracy scores for each baseline model on the test set:\n",
    "- Provides a baseline performance benchmark for comparison\n",
    "- Helps identify which model family (linear, tree-based, or kernel-based) performs best\n",
    "- Results inform hyperparameter tuning priorities\n",
    "- Establishes a reference point for measuring improvements after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52e75795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Baseline Model Results ---\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.4964\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.4964\n",
      "\n",
      "Model: XGBoost\n",
      "Accuracy: 0.4673\n",
      "\n",
      "Model: SVM\n",
      "Accuracy: 0.4576\n"
     ]
    }
   ],
   "source": [
    "# 6. Training Loop & Initial Evaluation\n",
    "print(\"--- Baseline Model Results ---\")\n",
    "for name, model in models.items():\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('classifier', model)])\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d872073",
   "metadata": {},
   "source": [
    "**Code Explanation :-**\n",
    "\n",
    "This cell reruns the baseline model training with more detailed output formatting:\n",
    "- Same four models (Logistic Regression, Random Forest, XGBoost, SVM)\n",
    "- Same pipeline and training approach as the previous cell\n",
    "- Prints results with clearer formatting: model name followed by accuracy score\n",
    "\n",
    "**Result Discussion :-**\n",
    "\n",
    "The output provides a clean, formatted display of baseline accuracies:\n",
    "- Shows performance summary for all four models side-by-side\n",
    "- Allows easy visual comparison of initial performance metrics\n",
    "- Establishes foundation before hyperparameter tuning\n",
    "- Results guide decisions on which models warrant further optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a29caa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comprehensive Hyperparameter Tuning ---\n",
      "\n",
      "Tuning Logistic Regression...\n",
      "Best CV Score: 0.5067\n",
      "Test Set Accuracy: 0.4939\n",
      "Best Params: {'classifier__C': 0.01, 'classifier__solver': 'liblinear'}\n",
      "\n",
      "Tuning Random Forest...\n",
      "Best CV Score: 0.5055\n",
      "Test Set Accuracy: 0.5400\n",
      "Best Params: {'classifier__max_depth': 20, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 50}\n",
      "\n",
      "Tuning XGBoost...\n",
      "Best CV Score: 0.5133\n",
      "Test Set Accuracy: 0.4673\n",
      "Best Params: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
      "\n",
      "Tuning SVM...\n",
      "Best CV Score: 0.5152\n",
      "Test Set Accuracy: 0.4625\n",
      "Best Params: {'classifier__C': 10, 'classifier__gamma': 'auto', 'classifier__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grids for all models\n",
    "# Note: 'classifier__' prefix is required to access model params inside the Pipeline\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__solver': ['liblinear']  # Good for small datasets\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__kernel': ['rbf', 'linear'],\n",
    "        'classifier__gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"--- Comprehensive Hyperparameter Tuning ---\")\n",
    "best_models = {}\n",
    "\n",
    "# Loop through each model and perform GridSearchCV\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model)])\n",
    "    \n",
    "    # Skip tuning if model not in our grid definition (safety check)\n",
    "    if name in param_grids:\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "        grid = GridSearchCV(pipeline, param_grids[name], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        best_score = grid.best_score_\n",
    "        best_params = grid.best_params_\n",
    "        test_acc = accuracy_score(y_test, grid.best_estimator_.predict(X_test))\n",
    "        \n",
    "        best_models[name] = {'test_accuracy': test_acc, 'best_params': best_params}\n",
    "        print(f\"Best CV Score: {best_score:.4f}\")\n",
    "        print(f\"Test Set Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459e717",
   "metadata": {},
   "source": [
    "**Code Explanation :-**\n",
    "This cell performs comprehensive hyperparameter tuning for all four models:\n",
    "1. **Defines Parameter Grids**: Specifies search spaces for each model's hyperparameters:\n",
    "   - Logistic Regression: Regularization strength (C), solver type\n",
    "   - Random Forest: Number of trees, max depth, min samples per split\n",
    "   - XGBoost: Number of trees, learning rate, max depth\n",
    "   - SVM: Regularization (C), kernel type, gamma parameter\n",
    "2. **GridSearchCV**: Exhaustively searches all parameter combinations using 3-fold cross-validation\n",
    "3. **Stores Results**: Saves best parameters and test accuracies for each model\n",
    "\n",
    "**Result Discussion :-**\n",
    "\n",
    "The output displays for each model:\n",
    "- **Best CV Score**: Cross-validation accuracy on training data (best fold average)\n",
    "- **Test Set Accuracy**: Generalization performance on unseen test data\n",
    "- **Best Params**: Optimal hyperparameter values discovered during grid search\n",
    "- Reveals which parameter configurations maximize predictive performance\n",
    "- Identifies if tuning improved baseline performance significantly\n",
    "- Enables selection of the best-performing model for final deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
